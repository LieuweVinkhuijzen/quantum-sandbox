take the minimum of several runs

greedilyOptimizeAllVertices()
    optimize all vertices

+ greedilyOptimizeAllEdges()
    for each edge (u,v), optimize the set {a,b}
    repeat until no more improvements are possible
+ optimize so that the evaluation function is not called so often
+ evaluateSolutionAfterFlippingPartition(graph, solution, partition, flips)
    returns the evaluation function after flipping the appropriate nodes
    this is much faster than calling evaluateSolution(), which iterates over all the graph's edges
- evaluateSolutionAfterFlippingPartition_super_fast
    in the case of large neighbourhood and few blocks (e.g., |nbh| = 200, numBlocks = 10, which is quite common)
    we can evaluate the thing faster by counting weighted edges, instead of evaluating all the edges
    complexity becomes only O(num_blocks ^ 2) instead of O(|nbh|)
    - use numpy to allocate fast arrays
    - use a persistent array, instead of one that is reallocated 

- try out a simple heuristic:
    choose a random vertex
    construct its neighbourhood
    flip a random set of vertices, with probability p = 0.5
    repeat some number of times. take the best outcome.
    Observation. This is not much less directed than the brute force attack involving partitions;
    it is, however, much simpler.
    We can play around with 3 parameters: the flip probability, the neighbourhood size, and the budget per neighbourhood

- modify the above heuristic:
    get a random neighbourhood
    flip a random set of vertices, with p=0.5
    optimize each edge within the neighbourhood, after this flip
    compute utility after both these operations
    choose the best candidate
    This way, we are not bound by investigating exactly 2^n options
    I guess the optimal schedule starts with the neighbourhood first is the whole graph, with p=0.5
        i.e., choose a couple of completely random, independent samples
    then slowly decrease the neighbourhood size, decrease p, and increase the budget per neighbourhood

- optimize a line in the graph
    FRom a starting vertex, perform depth-first search to get a line, i.e., a set of vertices v1, v2 ... vm such that only v(i)--v(i+1) are edges
    

- optimize a biclique

+ refactor solve_max_cut so there are 2 possible optimization steps:
    1. optimize neighbourhood
    2. shake up and then optimize neighbourhood
    + refactor so the code is a bit more neat

- use bandit theory to make a decision one way or the other, given the budget

Idea for an algorithm:
    Get a small set of vertices S as reference points, |S|=k
    for each vertex, compute the distances to these reference points
    this yields a vector in Z^k for each vector, it's a feature vector
    categorize each vertex in block 0 or block 1 depending on whether its feature vector contains majority even or majority odd numbers

if a solution is edge-optimal, then it is necessarily vertex-optimal

evolutionary algorithm: start with many instances; slowly kill of bad ones

draw graphs / plots of performance on various graphs, of various algorithms

execute D-wave on several of these instances

make an instance that can be executed well on D-Wave's architecture

- refactor a bunch of code to use networkx.graph
- take a random subset of a Chimera graph

find out how to map instances to the D-Wave topology, bypassing D-Wave's own tools
purpose: design instances relying on D-Wave's topology, so that D-Wave gets the maximum possible opportunity to demonstrate quantum advantage

- on Chimera graphs of various sizes:
    - take a random subset; sample from the maximum cut using own best algorithms many times
    - submit it to D-Wave a few times
    - sort both by solution quality (i.e., by descending penalty)

draw conclusion:
    - Does D-Wave show quantum advantage for Max-Cut problem?
        - what is the quality of solutions D-Wave delivers?
            - on its own optimal topology?
            - on small instances?
            - on large instances?

algorithm to detect clusters
    A collection of sets of vertices is a good set of clusters when each vertex has more neighbours within its cluster than outside of it
    So, randomly divide the vertices into k clusters.
    Then, say that each vertex attracts its neighbours into its cluster with a force proportional to 1/num neighbours.
    That is, each vertex has like 100 points to spend, and it spends it equally on its neighbours, so its 5 neighborus each get 20 points in its direction
    So, each vertex receives in total 100 points from its neighbours asking it to become part of their clusters.
    Each vertex then chooses to become part of the cluster that asks for it the most
    This obviously doesn't work in a bipartite graph very well, if the clusters are initialized as the 2 parts...
    perhaps: conditioned on dividing the vertices into k equal-size clusters, optimize the penalty function which assigns 1 penalty to every inter-cluster edge

once we have clusters, we can optimize each cluster very quickly; then we can optimzie inter-cluster relations later

LOW PRIORITY

fractalPartition()
    make a partition by setting ahead of time the sizes of the blocks as 1,2,4,8,16, etc
    fill each block with randomly chosen vertices

DONE

+ def optimizeLocal()
    optimize a local patch of the solution:
    choose a random node, and several nodes in its neighbourhood
    optimize this set

+ def optimizePartition(graph, solution, partition)
    optimize only the vertices in the partition
    the partition need not cover all vertices

+ optimizeLocal2()
    S := choose a random vertex and some number of nearby vertices
    P := divide S into k partitions
    optimizePartition(P)

